{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = '../data/processed'\n",
    "DATASET_TRAIN = DATASET_ROOT + '/train'\n",
    "DATASET_TEST = DATASET_ROOT + '/test'\n",
    "\n",
    "SAVED_MODELS_PATH = '../saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1615 images belonging to 3 classes.\n",
      "Found 283 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        DATASET_TRAIN,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        DATASET_TEST,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def vgg16_based_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(150, 150, 3)))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=x, name='vgg16_based')\n",
    "    return model\n",
    "\n",
    "\n",
    "def residual_block(x, filters_num):\n",
    "    x_residual = x\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters_num, (3, 3), padding='same')(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters_num, (3, 3), padding='same')(x)\n",
    "    \n",
    "    x = tf.keras.layers.add([x, x_residual])\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(x, filters_num):\n",
    "    x = Conv2D(filters_num, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_simple_model():\n",
    "    model_input = Input(shape=(150, 150, 3))\n",
    "    x = BatchNormalization()(model_input)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    \n",
    "    x = downsample(x, 128)\n",
    "    \n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    \n",
    "    x = downsample(x, 128)\n",
    "    \n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    \n",
    "    x = downsample(x, 64)\n",
    "    \n",
    "#     x = residual_block(x, 64)\n",
    "#     x = residual_block(x, 64)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=x, name='simple_residual')\n",
    "    return model\n",
    "              \n",
    "    \n",
    "model = resnet_simple_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_residual\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 150, 150, 3)  12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 150, 150, 64) 1792        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 150, 150, 64) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 150, 150, 64) 256         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 150, 150, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 150, 150, 64) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 150, 150, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150, 150, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 150, 150, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 150, 150, 64) 0           conv2d_2[0][0]                   \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 150, 150, 64) 256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 150, 150, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 150, 150, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 150, 150, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 150, 150, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 150, 150, 64) 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 150, 150, 64) 0           conv2d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 75, 75, 128)  131200      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 75, 75, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 75, 75, 128)  512         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 75, 75, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 75, 75, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 75, 75, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 75, 75, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 75, 75, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 75, 75, 128)  0           conv2d_7[0][0]                   \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 75, 75, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 75, 75, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 75, 75, 128)  147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 75, 75, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 75, 75, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 75, 75, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 75, 75, 128)  0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 38, 128)  262272      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 38, 38, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 38, 38, 128)  512         batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 38, 38, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 38, 38, 128)  147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 38, 38, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 38, 38, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 38, 38, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 38, 38, 128)  0           conv2d_12[0][0]                  \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 38, 38, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 38, 38, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 38, 38, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 38, 38, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 38, 38, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 38, 38, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 38, 38, 128)  0           conv2d_14[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 19, 19, 64)   131136      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 19, 19, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 23104)        0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1478720     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            195         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,340,111\n",
      "Trainable params: 3,336,905\n",
      "Non-trainable params: 3,206\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time_now = datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "path_to_save = SAVED_MODELS_PATH + '/' + model.name + '_' + time_now\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(path_to_save, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-0071fc268ffb>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 100 steps, validate for 17 steps\n",
      "Epoch 1/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.5186 - accuracy: 0.5092WARNING:tensorflow:From /home/jakub/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 110s 1s/step - loss: 1.5162 - accuracy: 0.5103 - val_loss: 2.3478 - val_accuracy: 0.2978\n",
      "Epoch 2/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.9705 - accuracy: 0.5483INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.9703 - accuracy: 0.5485 - val_loss: 0.9486 - val_accuracy: 0.5625\n",
      "Epoch 3/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.9217 - accuracy: 0.5472 - val_loss: 0.9532 - val_accuracy: 0.6360\n",
      "Epoch 4/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8856 - accuracy: 0.6033INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.8854 - accuracy: 0.6035 - val_loss: 0.8266 - val_accuracy: 0.5846\n",
      "Epoch 5/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8381 - accuracy: 0.6115INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.8390 - accuracy: 0.6104 - val_loss: 0.7925 - val_accuracy: 0.6912\n",
      "Epoch 6/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.8077 - accuracy: 0.6348 - val_loss: 0.8035 - val_accuracy: 0.6765\n",
      "Epoch 7/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8516 - accuracy: 0.6235INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.8534 - accuracy: 0.6229 - val_loss: 0.7889 - val_accuracy: 0.7206\n",
      "Epoch 8/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8329 - accuracy: 0.6077INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.8315 - accuracy: 0.6073 - val_loss: 0.7274 - val_accuracy: 0.7096\n",
      "Epoch 9/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.7881 - accuracy: 0.6429 - val_loss: 0.7753 - val_accuracy: 0.6875\n",
      "Epoch 10/1000000\n",
      "100/100 [==============================] - 94s 942ms/step - loss: 0.8433 - accuracy: 0.6191 - val_loss: 3.9797 - val_accuracy: 0.4926\n",
      "Epoch 11/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.8416 - accuracy: 0.6054 - val_loss: 0.8230 - val_accuracy: 0.6949\n",
      "Epoch 12/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.7856 - accuracy: 0.6354 - val_loss: 0.7451 - val_accuracy: 0.6654\n",
      "Epoch 13/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8210 - accuracy: 0.6488INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.8184 - accuracy: 0.6498 - val_loss: 0.7112 - val_accuracy: 0.7059\n",
      "Epoch 14/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.8076 - accuracy: 0.6735 - val_loss: 0.8362 - val_accuracy: 0.6801\n",
      "Epoch 15/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.7922 - accuracy: 0.6698 - val_loss: 0.7597 - val_accuracy: 0.7316\n",
      "Epoch 16/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.7885 - accuracy: 0.6473 - val_loss: 0.8191 - val_accuracy: 0.6618\n",
      "Epoch 17/1000000\n",
      "100/100 [==============================] - 94s 942ms/step - loss: 0.7841 - accuracy: 0.6635 - val_loss: 0.8231 - val_accuracy: 0.6544\n",
      "Epoch 18/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.8083 - accuracy: 0.6654 - val_loss: 0.7352 - val_accuracy: 0.7096\n",
      "Epoch 19/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.8362 - accuracy: 0.6542 - val_loss: 0.7170 - val_accuracy: 0.6875\n",
      "Epoch 20/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7784 - accuracy: 0.6589INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.7808 - accuracy: 0.6585 - val_loss: 0.7023 - val_accuracy: 0.7426\n",
      "Epoch 21/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.7823 - accuracy: 0.6654 - val_loss: 0.7838 - val_accuracy: 0.7132\n",
      "Epoch 22/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7910 - accuracy: 0.6667 - val_loss: 0.7173 - val_accuracy: 0.7353\n",
      "Epoch 23/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7853 - accuracy: 0.6633INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.7862 - accuracy: 0.6617 - val_loss: 0.6917 - val_accuracy: 0.7353\n",
      "Epoch 24/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7906 - accuracy: 0.6735 - val_loss: 0.7357 - val_accuracy: 0.7132\n",
      "Epoch 25/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.7709 - accuracy: 0.6473 - val_loss: 0.7313 - val_accuracy: 0.7243\n",
      "Epoch 26/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.7781 - accuracy: 0.6504 - val_loss: 0.6986 - val_accuracy: 0.7243\n",
      "Epoch 27/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.7808 - accuracy: 0.6710 - val_loss: 0.8811 - val_accuracy: 0.6287\n",
      "Epoch 28/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7877 - accuracy: 0.6642 - val_loss: 0.6932 - val_accuracy: 0.7243\n",
      "Epoch 29/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7908 - accuracy: 0.6671INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.7897 - accuracy: 0.6673 - val_loss: 0.6851 - val_accuracy: 0.7132\n",
      "Epoch 30/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7665 - accuracy: 0.6728INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.7671 - accuracy: 0.6735 - val_loss: 0.6816 - val_accuracy: 0.7316\n",
      "Epoch 31/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.7692 - accuracy: 0.6673 - val_loss: 0.6997 - val_accuracy: 0.7279\n",
      "Epoch 32/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.7710 - accuracy: 0.6729 - val_loss: 0.7187 - val_accuracy: 0.6949\n",
      "Epoch 33/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7569 - accuracy: 0.6598 - val_loss: 0.7126 - val_accuracy: 0.7279\n",
      "Epoch 34/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.7539 - accuracy: 0.6673 - val_loss: 0.7819 - val_accuracy: 0.6765\n",
      "Epoch 35/1000000\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 0.7511 - accuracy: 0.6617 - val_loss: 0.7115 - val_accuracy: 0.7243\n",
      "Epoch 36/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.7710 - accuracy: 0.6729 - val_loss: 0.7031 - val_accuracy: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.7880 - accuracy: 0.6679 - val_loss: 0.6972 - val_accuracy: 0.7500\n",
      "Epoch 38/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7460 - accuracy: 0.6723 - val_loss: 0.6917 - val_accuracy: 0.7243\n",
      "Epoch 39/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7550 - accuracy: 0.6604 - val_loss: 0.7911 - val_accuracy: 0.7169\n",
      "Epoch 40/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.7633 - accuracy: 0.6673 - val_loss: 0.7105 - val_accuracy: 0.7243\n",
      "Epoch 41/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.7800 - accuracy: 0.6529 - val_loss: 0.8002 - val_accuracy: 0.6691\n",
      "Epoch 42/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.7822 - accuracy: 0.6235 - val_loss: 0.7667 - val_accuracy: 0.6691\n",
      "Epoch 43/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.7600 - accuracy: 0.6604 - val_loss: 0.7147 - val_accuracy: 0.7426\n",
      "Epoch 44/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.7499 - accuracy: 0.6754 - val_loss: 0.7154 - val_accuracy: 0.7206\n",
      "Epoch 45/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.7530 - accuracy: 0.6785 - val_loss: 0.7383 - val_accuracy: 0.7353\n",
      "Epoch 46/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7410 - accuracy: 0.6660 - val_loss: 0.7009 - val_accuracy: 0.7426\n",
      "Epoch 47/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.7633 - accuracy: 0.6685 - val_loss: 0.6943 - val_accuracy: 0.7206\n",
      "Epoch 48/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7660 - accuracy: 0.6797INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.7680 - accuracy: 0.6798 - val_loss: 0.6813 - val_accuracy: 0.7243\n",
      "Epoch 49/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7466 - accuracy: 0.6911INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.7484 - accuracy: 0.6911 - val_loss: 0.6669 - val_accuracy: 0.7316\n",
      "Epoch 50/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.7328 - accuracy: 0.7029 - val_loss: 0.7630 - val_accuracy: 0.6765\n",
      "Epoch 51/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7281 - accuracy: 0.6667 - val_loss: 0.8036 - val_accuracy: 0.6765\n",
      "Epoch 52/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.7559 - accuracy: 0.6642 - val_loss: 0.7171 - val_accuracy: 0.6875\n",
      "Epoch 53/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.7545 - accuracy: 0.6592 - val_loss: 0.8047 - val_accuracy: 0.6250\n",
      "Epoch 54/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.7431 - accuracy: 0.6767 - val_loss: 0.6768 - val_accuracy: 0.7574\n",
      "Epoch 55/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.7370 - accuracy: 0.6742 - val_loss: 0.7348 - val_accuracy: 0.7243\n",
      "Epoch 56/1000000\n",
      "100/100 [==============================] - 96s 963ms/step - loss: 0.7263 - accuracy: 0.6817 - val_loss: 0.6950 - val_accuracy: 0.7426\n",
      "Epoch 57/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.7414 - accuracy: 0.6792 - val_loss: 0.6904 - val_accuracy: 0.7353\n",
      "Epoch 58/1000000\n",
      "100/100 [==============================] - 94s 942ms/step - loss: 0.7434 - accuracy: 0.6892 - val_loss: 0.6999 - val_accuracy: 0.7316\n",
      "Epoch 59/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.7316 - accuracy: 0.6823 - val_loss: 0.7684 - val_accuracy: 0.6985\n",
      "Epoch 60/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7372 - accuracy: 0.6779 - val_loss: 0.7078 - val_accuracy: 0.7279\n",
      "Epoch 61/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.7506 - accuracy: 0.6717 - val_loss: 0.7563 - val_accuracy: 0.6838\n",
      "Epoch 62/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.7411 - accuracy: 0.6854 - val_loss: 0.7556 - val_accuracy: 0.6801\n",
      "Epoch 63/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.7420 - accuracy: 0.6836 - val_loss: 0.7545 - val_accuracy: 0.7132\n",
      "Epoch 64/1000000\n",
      "100/100 [==============================] - 95s 945ms/step - loss: 0.7110 - accuracy: 0.6873 - val_loss: 0.8003 - val_accuracy: 0.7353\n",
      "Epoch 65/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.7570 - accuracy: 0.6710 - val_loss: 0.7059 - val_accuracy: 0.6985\n",
      "Epoch 66/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.7348 - accuracy: 0.6779 - val_loss: 0.6789 - val_accuracy: 0.7206\n",
      "Epoch 67/1000000\n",
      "100/100 [==============================] - 94s 945ms/step - loss: 0.7395 - accuracy: 0.6911 - val_loss: 0.6999 - val_accuracy: 0.7426\n",
      "Epoch 68/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.7374 - accuracy: 0.6698 - val_loss: 0.6948 - val_accuracy: 0.7206\n",
      "Epoch 69/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.7421 - accuracy: 0.6942 - val_loss: 0.6846 - val_accuracy: 0.7463\n",
      "Epoch 70/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.7244 - accuracy: 0.6873 - val_loss: 0.7368 - val_accuracy: 0.6949\n",
      "Epoch 71/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.7344 - accuracy: 0.6754 - val_loss: 0.7470 - val_accuracy: 0.7426\n",
      "Epoch 72/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.7165 - accuracy: 0.6961 - val_loss: 0.7369 - val_accuracy: 0.7426\n",
      "Epoch 73/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.7089 - accuracy: 0.6992 - val_loss: 0.7782 - val_accuracy: 0.6544\n",
      "Epoch 74/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.7279 - accuracy: 0.6717 - val_loss: 0.7079 - val_accuracy: 0.7132\n",
      "Epoch 75/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.7552 - accuracy: 0.6698 - val_loss: 0.6895 - val_accuracy: 0.7132\n",
      "Epoch 76/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.7319 - accuracy: 0.6829 - val_loss: 0.7174 - val_accuracy: 0.6985\n",
      "Epoch 77/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.7385 - accuracy: 0.6848 - val_loss: 0.7387 - val_accuracy: 0.7169\n",
      "Epoch 78/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.7629 - accuracy: 0.6904 - val_loss: 0.7415 - val_accuracy: 0.7243\n",
      "Epoch 79/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.7047 - accuracy: 0.6873 - val_loss: 0.7134 - val_accuracy: 0.7096\n",
      "Epoch 80/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.7097 - accuracy: 0.6986 - val_loss: 0.7088 - val_accuracy: 0.7279\n",
      "Epoch 81/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.7247 - accuracy: 0.6735 - val_loss: 0.7571 - val_accuracy: 0.7316\n",
      "Epoch 82/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.7073 - accuracy: 0.6879 - val_loss: 0.7045 - val_accuracy: 0.7279\n",
      "Epoch 83/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.6848 - accuracy: 0.7011 - val_loss: 0.7972 - val_accuracy: 0.6949\n",
      "Epoch 84/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.7098 - accuracy: 0.7079 - val_loss: 0.7952 - val_accuracy: 0.7353\n",
      "Epoch 85/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.7015 - accuracy: 0.6811 - val_loss: 0.7724 - val_accuracy: 0.7390\n",
      "Epoch 86/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.7168 - accuracy: 0.6942 - val_loss: 0.7325 - val_accuracy: 0.7132\n",
      "Epoch 87/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.7019 - accuracy: 0.6900 - val_loss: 1.0115 - val_accuracy: 0.6507\n",
      "Epoch 88/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7360 - accuracy: 0.6898 - val_loss: 0.7410 - val_accuracy: 0.7426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.7132 - accuracy: 0.6967 - val_loss: 0.7027 - val_accuracy: 0.7132\n",
      "Epoch 90/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.7025 - accuracy: 0.6898 - val_loss: 0.6897 - val_accuracy: 0.7059\n",
      "Epoch 91/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.7049 - accuracy: 0.7054 - val_loss: 0.6955 - val_accuracy: 0.7096\n",
      "Epoch 92/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.6899 - accuracy: 0.7061 - val_loss: 0.6999 - val_accuracy: 0.7132\n",
      "Epoch 93/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.7146 - accuracy: 0.6929 - val_loss: 0.6983 - val_accuracy: 0.7059\n",
      "Epoch 94/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.6908 - accuracy: 0.7061 - val_loss: 0.7704 - val_accuracy: 0.7316\n",
      "Epoch 95/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.7018INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.7029 - accuracy: 0.7023 - val_loss: 0.6574 - val_accuracy: 0.7279\n",
      "Epoch 96/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.7233INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6878 - accuracy: 0.7236 - val_loss: 0.6548 - val_accuracy: 0.7096\n",
      "Epoch 97/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7023 - accuracy: 0.7113INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.7034 - accuracy: 0.7104 - val_loss: 0.6527 - val_accuracy: 0.7353\n",
      "Epoch 98/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.6749 - accuracy: 0.7205 - val_loss: 0.6807 - val_accuracy: 0.7279\n",
      "Epoch 99/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.6985 - accuracy: 0.7017 - val_loss: 0.6936 - val_accuracy: 0.7169\n",
      "Epoch 100/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.6608 - accuracy: 0.7261 - val_loss: 0.6729 - val_accuracy: 0.6838\n",
      "Epoch 101/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.6798 - accuracy: 0.7217 - val_loss: 0.7061 - val_accuracy: 0.7096\n",
      "Epoch 102/1000000\n",
      "100/100 [==============================] - 94s 945ms/step - loss: 0.6861 - accuracy: 0.7023 - val_loss: 0.7570 - val_accuracy: 0.7132\n",
      "Epoch 103/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.6938 - accuracy: 0.7117 - val_loss: 0.6864 - val_accuracy: 0.7096\n",
      "Epoch 104/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.6657 - accuracy: 0.7042 - val_loss: 0.6702 - val_accuracy: 0.7059\n",
      "Epoch 105/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.6726 - accuracy: 0.7048 - val_loss: 0.6720 - val_accuracy: 0.7390\n",
      "Epoch 106/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.6750 - accuracy: 0.7242 - val_loss: 0.6950 - val_accuracy: 0.7022\n",
      "Epoch 107/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.6643 - accuracy: 0.7261 - val_loss: 0.6716 - val_accuracy: 0.7169\n",
      "Epoch 108/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.6668 - accuracy: 0.7154 - val_loss: 0.6806 - val_accuracy: 0.7206\n",
      "Epoch 109/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.6704 - accuracy: 0.7255 - val_loss: 0.6811 - val_accuracy: 0.7096\n",
      "Epoch 110/1000000\n",
      "100/100 [==============================] - 95s 945ms/step - loss: 0.6818 - accuracy: 0.7261 - val_loss: 0.6626 - val_accuracy: 0.7132\n",
      "Epoch 111/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.6388 - accuracy: 0.7330 - val_loss: 0.6910 - val_accuracy: 0.7537\n",
      "Epoch 112/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.6703 - accuracy: 0.7098 - val_loss: 0.7331 - val_accuracy: 0.6912\n",
      "Epoch 113/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6347 - accuracy: 0.7303INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.6378 - accuracy: 0.7292 - val_loss: 0.5918 - val_accuracy: 0.7574\n",
      "Epoch 114/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.6475 - accuracy: 0.7448 - val_loss: 0.7017 - val_accuracy: 0.7243\n",
      "Epoch 115/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.6585 - accuracy: 0.7179 - val_loss: 0.7248 - val_accuracy: 0.7426\n",
      "Epoch 116/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.6358 - accuracy: 0.7280 - val_loss: 0.6197 - val_accuracy: 0.7574\n",
      "Epoch 117/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.6596 - accuracy: 0.7255 - val_loss: 0.7156 - val_accuracy: 0.7096\n",
      "Epoch 118/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.6378 - accuracy: 0.7411 - val_loss: 0.6067 - val_accuracy: 0.7390\n",
      "Epoch 119/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.6131 - accuracy: 0.7598 - val_loss: 0.6554 - val_accuracy: 0.7500\n",
      "Epoch 120/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.6212 - accuracy: 0.7367 - val_loss: 0.6923 - val_accuracy: 0.7426\n",
      "Epoch 121/1000000\n",
      "100/100 [==============================] - 94s 938ms/step - loss: 0.6547 - accuracy: 0.7317 - val_loss: 0.6448 - val_accuracy: 0.7132\n",
      "Epoch 122/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.6432 - accuracy: 0.7455 - val_loss: 0.6003 - val_accuracy: 0.7500\n",
      "Epoch 123/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.6066 - accuracy: 0.7667 - val_loss: 0.6587 - val_accuracy: 0.7390\n",
      "Epoch 124/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.6176 - accuracy: 0.7436 - val_loss: 0.6450 - val_accuracy: 0.7574\n",
      "Epoch 125/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.5834 - accuracy: 0.7567 - val_loss: 0.7136 - val_accuracy: 0.7574\n",
      "Epoch 126/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.6340 - accuracy: 0.7398 - val_loss: 0.7194 - val_accuracy: 0.7390\n",
      "Epoch 127/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6220 - accuracy: 0.7555INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.6205 - accuracy: 0.7567 - val_loss: 0.5852 - val_accuracy: 0.7721\n",
      "Epoch 128/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.6045 - accuracy: 0.7530 - val_loss: 0.7168 - val_accuracy: 0.6985\n",
      "Epoch 129/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.6123 - accuracy: 0.7542 - val_loss: 0.6928 - val_accuracy: 0.7316\n",
      "Epoch 130/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.6283 - accuracy: 0.7480 - val_loss: 0.6716 - val_accuracy: 0.7169\n",
      "Epoch 131/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.6080 - accuracy: 0.7467 - val_loss: 0.6606 - val_accuracy: 0.7537\n",
      "Epoch 132/1000000\n",
      "100/100 [==============================] - 96s 962ms/step - loss: 0.5974 - accuracy: 0.7586 - val_loss: 0.6006 - val_accuracy: 0.7500\n",
      "Epoch 133/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.6170 - accuracy: 0.7473 - val_loss: 0.8299 - val_accuracy: 0.7610\n",
      "Epoch 134/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.6228 - accuracy: 0.7448 - val_loss: 0.6119 - val_accuracy: 0.7537\n",
      "Epoch 135/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.6064 - accuracy: 0.7498 - val_loss: 0.7261 - val_accuracy: 0.7647\n",
      "Epoch 136/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.5970 - accuracy: 0.7455 - val_loss: 0.7085 - val_accuracy: 0.7684\n",
      "Epoch 137/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 96s 959ms/step - loss: 0.5924 - accuracy: 0.7486 - val_loss: 0.6768 - val_accuracy: 0.7463\n",
      "Epoch 138/1000000\n",
      "100/100 [==============================] - 94s 940ms/step - loss: 0.5747 - accuracy: 0.7655 - val_loss: 0.6854 - val_accuracy: 0.7647\n",
      "Epoch 139/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5652 - accuracy: 0.7649 - val_loss: 0.6299 - val_accuracy: 0.7757\n",
      "Epoch 140/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.5812 - accuracy: 0.7649 - val_loss: 0.7564 - val_accuracy: 0.7390\n",
      "Epoch 141/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.5752 - accuracy: 0.7711 - val_loss: 0.7703 - val_accuracy: 0.7647\n",
      "Epoch 142/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5793 - accuracy: 0.7749 - val_loss: 0.6565 - val_accuracy: 0.7537\n",
      "Epoch 143/1000000\n",
      "100/100 [==============================] - 94s 940ms/step - loss: 0.5874 - accuracy: 0.7505 - val_loss: 0.6316 - val_accuracy: 0.7647\n",
      "Epoch 144/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.5807 - accuracy: 0.7655 - val_loss: 0.6893 - val_accuracy: 0.7463\n",
      "Epoch 145/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5857 - accuracy: 0.7561 - val_loss: 0.5880 - val_accuracy: 0.7574\n",
      "Epoch 146/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.5974 - accuracy: 0.7392 - val_loss: 0.7067 - val_accuracy: 0.7279\n",
      "Epoch 147/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.5898 - accuracy: 0.7617 - val_loss: 0.7330 - val_accuracy: 0.7647\n",
      "Epoch 148/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5775 - accuracy: 0.7674 - val_loss: 0.7730 - val_accuracy: 0.7537\n",
      "Epoch 149/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5603 - accuracy: 0.7736 - val_loss: 0.7070 - val_accuracy: 0.7463\n",
      "Epoch 150/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.5980 - accuracy: 0.7561 - val_loss: 0.5945 - val_accuracy: 0.7610\n",
      "Epoch 151/1000000\n",
      "100/100 [==============================] - 94s 942ms/step - loss: 0.5746 - accuracy: 0.7592 - val_loss: 0.5967 - val_accuracy: 0.7904\n",
      "Epoch 152/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.5225 - accuracy: 0.7886 - val_loss: 0.6644 - val_accuracy: 0.7500\n",
      "Epoch 153/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.6088 - accuracy: 0.7523 - val_loss: 0.8793 - val_accuracy: 0.7463\n",
      "Epoch 154/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.5620 - accuracy: 0.7642 - val_loss: 0.6802 - val_accuracy: 0.8051\n",
      "Epoch 155/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7625INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.5721 - accuracy: 0.7636 - val_loss: 0.5563 - val_accuracy: 0.7794\n",
      "Epoch 156/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.5358 - accuracy: 0.7742 - val_loss: 0.6955 - val_accuracy: 0.7721\n",
      "Epoch 157/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5630 - accuracy: 0.7711 - val_loss: 0.6349 - val_accuracy: 0.7831\n",
      "Epoch 158/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5323 - accuracy: 0.7774 - val_loss: 0.6309 - val_accuracy: 0.7574\n",
      "Epoch 159/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5444 - accuracy: 0.7917 - val_loss: 0.6380 - val_accuracy: 0.7868\n",
      "Epoch 160/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.5583 - accuracy: 0.7742 - val_loss: 0.6809 - val_accuracy: 0.7647\n",
      "Epoch 161/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.5649 - accuracy: 0.7711 - val_loss: 0.6762 - val_accuracy: 0.7941\n",
      "Epoch 162/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.5444 - accuracy: 0.7899 - val_loss: 0.6553 - val_accuracy: 0.7574\n",
      "Epoch 163/1000000\n",
      "100/100 [==============================] - 94s 936ms/step - loss: 0.5631 - accuracy: 0.7642 - val_loss: 0.7396 - val_accuracy: 0.7721\n",
      "Epoch 164/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.5514 - accuracy: 0.7799 - val_loss: 0.6173 - val_accuracy: 0.7978\n",
      "Epoch 165/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.5332 - accuracy: 0.7874 - val_loss: 0.6221 - val_accuracy: 0.7684\n",
      "Epoch 166/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.5373 - accuracy: 0.7899 - val_loss: 0.6290 - val_accuracy: 0.7500\n",
      "Epoch 167/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.5747 - accuracy: 0.7611 - val_loss: 0.6314 - val_accuracy: 0.7390\n",
      "Epoch 168/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.5494 - accuracy: 0.7736 - val_loss: 0.6874 - val_accuracy: 0.7721\n",
      "Epoch 169/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.5258 - accuracy: 0.7774 - val_loss: 0.6266 - val_accuracy: 0.7684\n",
      "Epoch 170/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5285 - accuracy: 0.7899 - val_loss: 0.6054 - val_accuracy: 0.7463\n",
      "Epoch 171/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5477 - accuracy: 0.7761 - val_loss: 0.6293 - val_accuracy: 0.7757\n",
      "Epoch 172/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.5364 - accuracy: 0.7736 - val_loss: 0.7601 - val_accuracy: 0.7390\n",
      "Epoch 173/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5266 - accuracy: 0.7811 - val_loss: 0.6281 - val_accuracy: 0.7831\n",
      "Epoch 174/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.5061 - accuracy: 0.7992 - val_loss: 0.7096 - val_accuracy: 0.7684\n",
      "Epoch 175/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.5101 - accuracy: 0.7874 - val_loss: 0.6301 - val_accuracy: 0.7721\n",
      "Epoch 176/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5477 - accuracy: 0.7917 - val_loss: 0.6029 - val_accuracy: 0.7610\n",
      "Epoch 177/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.5456 - accuracy: 0.7774 - val_loss: 0.6395 - val_accuracy: 0.7794\n",
      "Epoch 178/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.5252 - accuracy: 0.7761 - val_loss: 0.6516 - val_accuracy: 0.7647\n",
      "Epoch 179/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.5193 - accuracy: 0.7994 - val_loss: 0.6787 - val_accuracy: 0.7868\n",
      "Epoch 180/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.5369 - accuracy: 0.7767 - val_loss: 0.7922 - val_accuracy: 0.7941\n",
      "Epoch 181/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.5145 - accuracy: 0.7861 - val_loss: 0.6009 - val_accuracy: 0.8125\n",
      "Epoch 182/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5135 - accuracy: 0.7930 - val_loss: 0.6595 - val_accuracy: 0.7537\n",
      "Epoch 183/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.4976 - accuracy: 0.7969 - val_loss: 0.6946 - val_accuracy: 0.7426\n",
      "Epoch 184/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5155 - accuracy: 0.7849 - val_loss: 0.5665 - val_accuracy: 0.8015\n",
      "Epoch 185/1000000\n",
      "100/100 [==============================] - 94s 939ms/step - loss: 0.5131 - accuracy: 0.7892 - val_loss: 0.7473 - val_accuracy: 0.7610\n",
      "Epoch 186/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5091 - accuracy: 0.8011 - val_loss: 0.6013 - val_accuracy: 0.7426\n",
      "Epoch 187/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7928INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.5099 - accuracy: 0.7942 - val_loss: 0.5518 - val_accuracy: 0.8088\n",
      "Epoch 188/1000000\n",
      "100/100 [==============================] - 94s 945ms/step - loss: 0.5281 - accuracy: 0.7761 - val_loss: 0.6938 - val_accuracy: 0.7868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5065 - accuracy: 0.7899 - val_loss: 0.5754 - val_accuracy: 0.7647\n",
      "Epoch 190/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5343 - accuracy: 0.7842 - val_loss: 0.6904 - val_accuracy: 0.8015\n",
      "Epoch 191/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.4875 - accuracy: 0.8011 - val_loss: 0.6087 - val_accuracy: 0.8015\n",
      "Epoch 192/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.4857 - accuracy: 0.8024 - val_loss: 0.8640 - val_accuracy: 0.7721\n",
      "Epoch 193/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.4930 - accuracy: 0.8136 - val_loss: 0.5721 - val_accuracy: 0.7868\n",
      "Epoch 194/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.5241 - accuracy: 0.7880 - val_loss: 0.6091 - val_accuracy: 0.7647\n",
      "Epoch 195/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.4754 - accuracy: 0.8118 - val_loss: 0.6778 - val_accuracy: 0.6912\n",
      "Epoch 196/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.5012 - accuracy: 0.7924 - val_loss: 0.6172 - val_accuracy: 0.7831\n",
      "Epoch 197/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.4828 - accuracy: 0.7924 - val_loss: 0.6145 - val_accuracy: 0.7426\n",
      "Epoch 198/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4916 - accuracy: 0.7880 - val_loss: 0.6742 - val_accuracy: 0.7610\n",
      "Epoch 199/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.4970 - accuracy: 0.7942 - val_loss: 0.6435 - val_accuracy: 0.7757\n",
      "Epoch 200/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.5022 - accuracy: 0.7899 - val_loss: 0.7156 - val_accuracy: 0.7794\n",
      "Epoch 201/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4986 - accuracy: 0.7880 - val_loss: 0.5700 - val_accuracy: 0.7757\n",
      "Epoch 202/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.4757 - accuracy: 0.7905 - val_loss: 0.7321 - val_accuracy: 0.7941\n",
      "Epoch 203/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.5030 - accuracy: 0.7974 - val_loss: 0.5988 - val_accuracy: 0.7500\n",
      "Epoch 204/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.4644 - accuracy: 0.8168 - val_loss: 0.5678 - val_accuracy: 0.7647\n",
      "Epoch 205/1000000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.8117INFO:tensorflow:Assets written to: ../saved_models/simple_residual_2020_05_01__21_58_00/assets\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.4459 - accuracy: 0.8105 - val_loss: 0.5180 - val_accuracy: 0.8309\n",
      "Epoch 206/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.4709 - accuracy: 0.8055 - val_loss: 0.7026 - val_accuracy: 0.7904\n",
      "Epoch 207/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.4791 - accuracy: 0.7917 - val_loss: 0.8547 - val_accuracy: 0.8125\n",
      "Epoch 208/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.4894 - accuracy: 0.8061 - val_loss: 0.5820 - val_accuracy: 0.7941\n",
      "Epoch 209/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.4549 - accuracy: 0.8118 - val_loss: 0.7971 - val_accuracy: 0.7757\n",
      "Epoch 210/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.4634 - accuracy: 0.8074 - val_loss: 0.7588 - val_accuracy: 0.7978\n",
      "Epoch 211/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4479 - accuracy: 0.8111 - val_loss: 0.7401 - val_accuracy: 0.8088\n",
      "Epoch 212/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.4314 - accuracy: 0.8143 - val_loss: 0.7138 - val_accuracy: 0.8051\n",
      "Epoch 213/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.4627 - accuracy: 0.8068 - val_loss: 0.7371 - val_accuracy: 0.7904\n",
      "Epoch 214/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.4601 - accuracy: 0.8205 - val_loss: 0.5962 - val_accuracy: 0.8051\n",
      "Epoch 215/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.4738 - accuracy: 0.8068 - val_loss: 0.6090 - val_accuracy: 0.7868\n",
      "Epoch 216/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.4442 - accuracy: 0.8180 - val_loss: 0.7541 - val_accuracy: 0.8125\n",
      "Epoch 217/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.4403 - accuracy: 0.8136 - val_loss: 0.6895 - val_accuracy: 0.8125\n",
      "Epoch 218/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.4520 - accuracy: 0.8199 - val_loss: 0.6634 - val_accuracy: 0.8199\n",
      "Epoch 219/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.4380 - accuracy: 0.8174 - val_loss: 0.7457 - val_accuracy: 0.7794\n",
      "Epoch 220/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4347 - accuracy: 0.8243 - val_loss: 0.6604 - val_accuracy: 0.7978\n",
      "Epoch 221/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.4629 - accuracy: 0.8055 - val_loss: 0.6414 - val_accuracy: 0.8088\n",
      "Epoch 222/1000000\n",
      "100/100 [==============================] - 94s 939ms/step - loss: 0.4334 - accuracy: 0.8211 - val_loss: 0.6688 - val_accuracy: 0.7794\n",
      "Epoch 223/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.4465 - accuracy: 0.8249 - val_loss: 0.8596 - val_accuracy: 0.7978\n",
      "Epoch 224/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.4293 - accuracy: 0.8211 - val_loss: 0.7354 - val_accuracy: 0.8015\n",
      "Epoch 225/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.4529 - accuracy: 0.8205 - val_loss: 0.5557 - val_accuracy: 0.8162\n",
      "Epoch 226/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.4370 - accuracy: 0.8174 - val_loss: 0.5629 - val_accuracy: 0.8051\n",
      "Epoch 227/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4328 - accuracy: 0.8243 - val_loss: 0.6359 - val_accuracy: 0.7684\n",
      "Epoch 228/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.4306 - accuracy: 0.8268 - val_loss: 0.6003 - val_accuracy: 0.8051\n",
      "Epoch 229/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4165 - accuracy: 0.8255 - val_loss: 0.6526 - val_accuracy: 0.8162\n",
      "Epoch 230/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4215 - accuracy: 0.8268 - val_loss: 0.8060 - val_accuracy: 0.8051\n",
      "Epoch 231/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.4280 - accuracy: 0.8261 - val_loss: 0.6009 - val_accuracy: 0.7831\n",
      "Epoch 232/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.4391 - accuracy: 0.8211 - val_loss: 0.6058 - val_accuracy: 0.8162\n",
      "Epoch 233/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.4156 - accuracy: 0.8249 - val_loss: 0.6638 - val_accuracy: 0.7941\n",
      "Epoch 234/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.4094 - accuracy: 0.8318 - val_loss: 0.6274 - val_accuracy: 0.8051\n",
      "Epoch 235/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3829 - accuracy: 0.8418 - val_loss: 0.7311 - val_accuracy: 0.7868\n",
      "Epoch 236/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.3869 - accuracy: 0.8474 - val_loss: 0.7314 - val_accuracy: 0.8199\n",
      "Epoch 237/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4058 - accuracy: 0.8324 - val_loss: 0.7426 - val_accuracy: 0.8015\n",
      "Epoch 238/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.4096 - accuracy: 0.8386 - val_loss: 0.6435 - val_accuracy: 0.7904\n",
      "Epoch 239/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.3748 - accuracy: 0.8512 - val_loss: 0.7277 - val_accuracy: 0.8051\n",
      "Epoch 240/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.3744 - accuracy: 0.8405 - val_loss: 0.6030 - val_accuracy: 0.8015\n",
      "Epoch 241/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.4041 - accuracy: 0.8424 - val_loss: 0.7420 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.3862 - accuracy: 0.8493 - val_loss: 0.6332 - val_accuracy: 0.8235\n",
      "Epoch 243/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.4030 - accuracy: 0.8412 - val_loss: 0.8430 - val_accuracy: 0.8088\n",
      "Epoch 244/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.4359 - accuracy: 0.8361 - val_loss: 0.6293 - val_accuracy: 0.8235\n",
      "Epoch 245/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.3925 - accuracy: 0.8368 - val_loss: 0.9475 - val_accuracy: 0.8015\n",
      "Epoch 246/1000000\n",
      "100/100 [==============================] - 94s 940ms/step - loss: 0.3913 - accuracy: 0.8424 - val_loss: 0.6766 - val_accuracy: 0.8125\n",
      "Epoch 247/1000000\n",
      "100/100 [==============================] - 94s 937ms/step - loss: 0.3815 - accuracy: 0.8450 - val_loss: 0.6253 - val_accuracy: 0.8162\n",
      "Epoch 248/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.3828 - accuracy: 0.8468 - val_loss: 0.7318 - val_accuracy: 0.7978\n",
      "Epoch 249/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.4054 - accuracy: 0.8424 - val_loss: 0.7480 - val_accuracy: 0.8088\n",
      "Epoch 250/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.3619 - accuracy: 0.8574 - val_loss: 0.7304 - val_accuracy: 0.8199\n",
      "Epoch 251/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.3909 - accuracy: 0.8355 - val_loss: 0.6881 - val_accuracy: 0.8125\n",
      "Epoch 252/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.3656 - accuracy: 0.8462 - val_loss: 0.9451 - val_accuracy: 0.8199\n",
      "Epoch 253/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3617 - accuracy: 0.8568 - val_loss: 0.7692 - val_accuracy: 0.7978\n",
      "Epoch 254/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.3724 - accuracy: 0.8468 - val_loss: 0.8896 - val_accuracy: 0.7941\n",
      "Epoch 255/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.3835 - accuracy: 0.8443 - val_loss: 0.6692 - val_accuracy: 0.8456\n",
      "Epoch 256/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.3506 - accuracy: 0.8618 - val_loss: 0.7312 - val_accuracy: 0.8272\n",
      "Epoch 257/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.3724 - accuracy: 0.8605 - val_loss: 0.6945 - val_accuracy: 0.8419\n",
      "Epoch 258/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.3466 - accuracy: 0.8605 - val_loss: 0.8598 - val_accuracy: 0.8088\n",
      "Epoch 259/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3471 - accuracy: 0.8618 - val_loss: 0.7321 - val_accuracy: 0.8088\n",
      "Epoch 260/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.3605 - accuracy: 0.8543 - val_loss: 0.9911 - val_accuracy: 0.8051\n",
      "Epoch 261/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.3351 - accuracy: 0.8649 - val_loss: 1.1245 - val_accuracy: 0.8272\n",
      "Epoch 262/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.3352 - accuracy: 0.8705 - val_loss: 0.6961 - val_accuracy: 0.7721\n",
      "Epoch 263/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.3561 - accuracy: 0.8574 - val_loss: 0.6253 - val_accuracy: 0.8125\n",
      "Epoch 264/1000000\n",
      "100/100 [==============================] - 94s 942ms/step - loss: 0.3344 - accuracy: 0.8668 - val_loss: 0.6847 - val_accuracy: 0.7610\n",
      "Epoch 265/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.3850 - accuracy: 0.8537 - val_loss: 1.2113 - val_accuracy: 0.8015\n",
      "Epoch 266/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.3718 - accuracy: 0.8480 - val_loss: 0.7495 - val_accuracy: 0.8051\n",
      "Epoch 267/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.3485 - accuracy: 0.8674 - val_loss: 0.6315 - val_accuracy: 0.8235\n",
      "Epoch 268/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.3394 - accuracy: 0.8599 - val_loss: 0.7825 - val_accuracy: 0.7941\n",
      "Epoch 269/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.3449 - accuracy: 0.8593 - val_loss: 0.8082 - val_accuracy: 0.8272\n",
      "Epoch 270/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.3291 - accuracy: 0.8699 - val_loss: 0.6583 - val_accuracy: 0.7426\n",
      "Epoch 271/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3407 - accuracy: 0.8612 - val_loss: 0.6196 - val_accuracy: 0.7941\n",
      "Epoch 272/1000000\n",
      "100/100 [==============================] - 96s 961ms/step - loss: 0.3303 - accuracy: 0.8662 - val_loss: 0.6549 - val_accuracy: 0.8272\n",
      "Epoch 273/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.3391 - accuracy: 0.8674 - val_loss: 0.5503 - val_accuracy: 0.8235\n",
      "Epoch 274/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.3195 - accuracy: 0.8755 - val_loss: 0.6111 - val_accuracy: 0.8272\n",
      "Epoch 275/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.3545 - accuracy: 0.8649 - val_loss: 0.5598 - val_accuracy: 0.8015\n",
      "Epoch 276/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.3207 - accuracy: 0.8680 - val_loss: 0.6887 - val_accuracy: 0.7868\n",
      "Epoch 277/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.3324 - accuracy: 0.8662 - val_loss: 0.9158 - val_accuracy: 0.8088\n",
      "Epoch 278/1000000\n",
      "100/100 [==============================] - 95s 945ms/step - loss: 0.3129 - accuracy: 0.8737 - val_loss: 0.8632 - val_accuracy: 0.8272\n",
      "Epoch 279/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3142 - accuracy: 0.8868 - val_loss: 0.8324 - val_accuracy: 0.8235\n",
      "Epoch 280/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.2980 - accuracy: 0.8799 - val_loss: 0.8919 - val_accuracy: 0.8199\n",
      "Epoch 281/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.3164 - accuracy: 0.8687 - val_loss: 0.7318 - val_accuracy: 0.8125\n",
      "Epoch 282/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.3118 - accuracy: 0.8774 - val_loss: 0.7192 - val_accuracy: 0.8346\n",
      "Epoch 283/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.3255 - accuracy: 0.8662 - val_loss: 0.6766 - val_accuracy: 0.8235\n",
      "Epoch 284/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.3063 - accuracy: 0.8818 - val_loss: 0.7761 - val_accuracy: 0.8382\n",
      "Epoch 285/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.3163 - accuracy: 0.8787 - val_loss: 0.8206 - val_accuracy: 0.8125\n",
      "Epoch 286/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.3193 - accuracy: 0.8793 - val_loss: 0.6327 - val_accuracy: 0.8346\n",
      "Epoch 287/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3131 - accuracy: 0.8787 - val_loss: 0.6442 - val_accuracy: 0.8162\n",
      "Epoch 288/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.2832 - accuracy: 0.8906 - val_loss: 0.7745 - val_accuracy: 0.8015\n",
      "Epoch 289/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.2964 - accuracy: 0.8743 - val_loss: 0.8197 - val_accuracy: 0.7978\n",
      "Epoch 290/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.2860 - accuracy: 0.8881 - val_loss: 0.7199 - val_accuracy: 0.8272\n",
      "Epoch 291/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.2676 - accuracy: 0.9006 - val_loss: 1.0090 - val_accuracy: 0.8162\n",
      "Epoch 292/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.3022 - accuracy: 0.8881 - val_loss: 0.8194 - val_accuracy: 0.8088\n",
      "Epoch 293/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.3050 - accuracy: 0.8837 - val_loss: 0.7385 - val_accuracy: 0.7978\n",
      "Epoch 294/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2889 - accuracy: 0.8862 - val_loss: 1.0992 - val_accuracy: 0.7941\n",
      "Epoch 295/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.2814 - accuracy: 0.8874 - val_loss: 0.6747 - val_accuracy: 0.8346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.2759 - accuracy: 0.8887 - val_loss: 0.7807 - val_accuracy: 0.8015\n",
      "Epoch 297/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.3113 - accuracy: 0.8705 - val_loss: 0.8274 - val_accuracy: 0.8125\n",
      "Epoch 298/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.2658 - accuracy: 0.8999 - val_loss: 0.6099 - val_accuracy: 0.7978\n",
      "Epoch 299/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.2695 - accuracy: 0.8949 - val_loss: 0.8326 - val_accuracy: 0.8346\n",
      "Epoch 300/1000000\n",
      "100/100 [==============================] - 94s 939ms/step - loss: 0.2964 - accuracy: 0.8799 - val_loss: 0.6871 - val_accuracy: 0.8199\n",
      "Epoch 301/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.2751 - accuracy: 0.8943 - val_loss: 0.9793 - val_accuracy: 0.8199\n",
      "Epoch 302/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.2639 - accuracy: 0.9043 - val_loss: 0.7513 - val_accuracy: 0.7868\n",
      "Epoch 303/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.2801 - accuracy: 0.8981 - val_loss: 0.7797 - val_accuracy: 0.7721\n",
      "Epoch 304/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.2748 - accuracy: 0.8937 - val_loss: 0.7922 - val_accuracy: 0.8199\n",
      "Epoch 305/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2628 - accuracy: 0.8968 - val_loss: 0.7621 - val_accuracy: 0.7831\n",
      "Epoch 306/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2850 - accuracy: 0.9024 - val_loss: 0.8277 - val_accuracy: 0.8125\n",
      "Epoch 307/1000000\n",
      "100/100 [==============================] - 94s 945ms/step - loss: 0.2873 - accuracy: 0.8893 - val_loss: 0.8139 - val_accuracy: 0.8125\n",
      "Epoch 308/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.2812 - accuracy: 0.8993 - val_loss: 0.8001 - val_accuracy: 0.8272\n",
      "Epoch 309/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.2350 - accuracy: 0.9143 - val_loss: 0.7330 - val_accuracy: 0.7831\n",
      "Epoch 310/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2395 - accuracy: 0.9099 - val_loss: 0.8505 - val_accuracy: 0.8199\n",
      "Epoch 311/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2643 - accuracy: 0.8899 - val_loss: 0.7683 - val_accuracy: 0.7684\n",
      "Epoch 312/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.2409 - accuracy: 0.9087 - val_loss: 0.6860 - val_accuracy: 0.8015\n",
      "Epoch 313/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.2742 - accuracy: 0.8899 - val_loss: 0.9313 - val_accuracy: 0.8088\n",
      "Epoch 314/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.2838 - accuracy: 0.8993 - val_loss: 0.9511 - val_accuracy: 0.8346\n",
      "Epoch 315/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.2353 - accuracy: 0.9149 - val_loss: 0.7989 - val_accuracy: 0.8309\n",
      "Epoch 316/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.2472 - accuracy: 0.9068 - val_loss: 1.0638 - val_accuracy: 0.8088\n",
      "Epoch 317/1000000\n",
      "100/100 [==============================] - 96s 958ms/step - loss: 0.2595 - accuracy: 0.9118 - val_loss: 0.8666 - val_accuracy: 0.8088\n",
      "Epoch 318/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.2552 - accuracy: 0.9006 - val_loss: 0.7198 - val_accuracy: 0.8382\n",
      "Epoch 319/1000000\n",
      "100/100 [==============================] - 95s 951ms/step - loss: 0.2459 - accuracy: 0.9106 - val_loss: 0.9494 - val_accuracy: 0.7904\n",
      "Epoch 320/1000000\n",
      "100/100 [==============================] - 94s 944ms/step - loss: 0.2493 - accuracy: 0.9056 - val_loss: 0.9250 - val_accuracy: 0.8088\n",
      "Epoch 321/1000000\n",
      "100/100 [==============================] - 96s 960ms/step - loss: 0.2446 - accuracy: 0.9193 - val_loss: 0.7800 - val_accuracy: 0.8309\n",
      "Epoch 322/1000000\n",
      "100/100 [==============================] - 96s 959ms/step - loss: 0.2374 - accuracy: 0.9131 - val_loss: 0.8370 - val_accuracy: 0.8162\n",
      "Epoch 323/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.2264 - accuracy: 0.9187 - val_loss: 0.8793 - val_accuracy: 0.8199\n",
      "Epoch 324/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2430 - accuracy: 0.9131 - val_loss: 0.8917 - val_accuracy: 0.8346\n",
      "Epoch 325/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2359 - accuracy: 0.9124 - val_loss: 0.9025 - val_accuracy: 0.8051\n",
      "Epoch 326/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.2781 - accuracy: 0.9024 - val_loss: 0.9468 - val_accuracy: 0.7831\n",
      "Epoch 327/1000000\n",
      "100/100 [==============================] - 94s 941ms/step - loss: 0.2324 - accuracy: 0.9137 - val_loss: 0.8538 - val_accuracy: 0.8456\n",
      "Epoch 328/1000000\n",
      "100/100 [==============================] - 95s 952ms/step - loss: 0.2077 - accuracy: 0.9306 - val_loss: 0.6319 - val_accuracy: 0.8125\n",
      "Epoch 329/1000000\n",
      "100/100 [==============================] - 95s 948ms/step - loss: 0.2175 - accuracy: 0.9199 - val_loss: 0.7913 - val_accuracy: 0.8309\n",
      "Epoch 330/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.2894 - accuracy: 0.9037 - val_loss: 0.6713 - val_accuracy: 0.8235\n",
      "Epoch 331/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2434 - accuracy: 0.9106 - val_loss: 0.6299 - val_accuracy: 0.8309\n",
      "Epoch 332/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.1984 - accuracy: 0.9318 - val_loss: 0.7885 - val_accuracy: 0.8199\n",
      "Epoch 333/1000000\n",
      "100/100 [==============================] - 93s 934ms/step - loss: 0.2281 - accuracy: 0.9174 - val_loss: 0.6644 - val_accuracy: 0.8529\n",
      "Epoch 334/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.2252 - accuracy: 0.9206 - val_loss: 0.6779 - val_accuracy: 0.8346\n",
      "Epoch 335/1000000\n",
      "100/100 [==============================] - 94s 943ms/step - loss: 0.2598 - accuracy: 0.9156 - val_loss: 0.7892 - val_accuracy: 0.8419\n",
      "Epoch 336/1000000\n",
      "100/100 [==============================] - 95s 947ms/step - loss: 0.2480 - accuracy: 0.9081 - val_loss: 0.9884 - val_accuracy: 0.8456\n",
      "Epoch 337/1000000\n",
      "100/100 [==============================] - 96s 955ms/step - loss: 0.2235 - accuracy: 0.9174 - val_loss: 0.8199 - val_accuracy: 0.8346\n",
      "Epoch 338/1000000\n",
      "100/100 [==============================] - 95s 945ms/step - loss: 0.2022 - accuracy: 0.9287 - val_loss: 0.7191 - val_accuracy: 0.8382\n",
      "Epoch 339/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.2573 - accuracy: 0.9112 - val_loss: 0.7105 - val_accuracy: 0.8529\n",
      "Epoch 340/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2169 - accuracy: 0.9149 - val_loss: 0.8463 - val_accuracy: 0.8346\n",
      "Epoch 341/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2065 - accuracy: 0.9212 - val_loss: 0.8188 - val_accuracy: 0.8272\n",
      "Epoch 342/1000000\n",
      "100/100 [==============================] - 96s 956ms/step - loss: 0.1740 - accuracy: 0.9375 - val_loss: 0.7652 - val_accuracy: 0.8493\n",
      "Epoch 343/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.2114 - accuracy: 0.9199 - val_loss: 0.6348 - val_accuracy: 0.8382\n",
      "Epoch 344/1000000\n",
      "100/100 [==============================] - 95s 949ms/step - loss: 0.1937 - accuracy: 0.9250 - val_loss: 0.7746 - val_accuracy: 0.8346\n",
      "Epoch 345/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.2120 - accuracy: 0.9256 - val_loss: 0.9010 - val_accuracy: 0.8382\n",
      "Epoch 346/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.2008 - accuracy: 0.9293 - val_loss: 0.7634 - val_accuracy: 0.8456\n",
      "Epoch 347/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.1910 - accuracy: 0.9293 - val_loss: 1.0212 - val_accuracy: 0.8493\n",
      "Epoch 348/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2032 - accuracy: 0.9268 - val_loss: 1.0635 - val_accuracy: 0.7206\n",
      "Epoch 349/1000000\n",
      "100/100 [==============================] - 96s 957ms/step - loss: 0.2161 - accuracy: 0.9225 - val_loss: 0.7079 - val_accuracy: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.1855 - accuracy: 0.9287 - val_loss: 0.9018 - val_accuracy: 0.8235\n",
      "Epoch 351/1000000\n",
      "100/100 [==============================] - 95s 953ms/step - loss: 0.1926 - accuracy: 0.9337 - val_loss: 1.0203 - val_accuracy: 0.8235\n",
      "Epoch 352/1000000\n",
      "100/100 [==============================] - 94s 939ms/step - loss: 0.1954 - accuracy: 0.9243 - val_loss: 0.9390 - val_accuracy: 0.8309\n",
      "Epoch 353/1000000\n",
      "100/100 [==============================] - 95s 946ms/step - loss: 0.1979 - accuracy: 0.9362 - val_loss: 0.9575 - val_accuracy: 0.8162\n",
      "Epoch 354/1000000\n",
      "100/100 [==============================] - 95s 954ms/step - loss: 0.2087 - accuracy: 0.9275 - val_loss: 1.2382 - val_accuracy: 0.7978\n",
      "Epoch 355/1000000\n",
      "100/100 [==============================] - 95s 950ms/step - loss: 0.2035 - accuracy: 0.9212 - val_loss: 0.9127 - val_accuracy: 0.7831\n",
      "Epoch 356/1000000\n",
      "100/100 [==============================] - 95s 955ms/step - loss: 0.2344 - accuracy: 0.9174 - val_loss: 0.7761 - val_accuracy: 0.8346\n",
      "Epoch 357/1000000\n",
      "100/100 [==============================] - 94s 940ms/step - loss: 0.1996 - accuracy: 0.9268 - val_loss: 0.6694 - val_accuracy: 0.8309\n",
      "Epoch 358/1000000\n",
      " 55/100 [===============>..............] - ETA: 50s - loss: 0.2144 - accuracy: 0.9224WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0071fc268ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         callbacks=[cp_callback])\n\u001b[0m",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/covid-19/env/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "        epochs=1000000,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=STEP_SIZE_VALID,\n",
    "        callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covid19': 0, 'normal': 1, 'pneumonia': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    SAVED_MODELS_PATH + '/' + 'simple_residual_2020_05_01__21_58_00', custom_objects=None, compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.39236259e-01, 3.45706604e-02, 1.26193032e-01],\n",
       "       [6.27018452e-01, 9.65304822e-02, 2.76450962e-01],\n",
       "       [1.92986384e-01, 4.96838987e-02, 7.57329702e-01],\n",
       "       [6.49531245e-01, 8.93131420e-02, 2.61155695e-01],\n",
       "       [4.56081122e-01, 1.56729087e-01, 3.87189806e-01],\n",
       "       [2.08050806e-05, 9.99507189e-01, 4.72011045e-04],\n",
       "       [1.73253730e-01, 2.12451726e-01, 6.14294529e-01],\n",
       "       [2.91299112e-02, 9.16204989e-01, 5.46650924e-02],\n",
       "       [2.78259702e-02, 8.08551311e-01, 1.63622722e-01],\n",
       "       [5.89504659e-01, 1.08902387e-01, 3.01592946e-01],\n",
       "       [9.79541719e-01, 2.96395877e-03, 1.74943041e-02],\n",
       "       [9.86485481e-01, 1.08893705e-03, 1.24255894e-02],\n",
       "       [8.39862134e-03, 9.30850625e-01, 6.07508272e-02],\n",
       "       [6.02468729e-01, 1.04577340e-01, 2.92953908e-01],\n",
       "       [7.94432104e-01, 1.11042578e-02, 1.94463596e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [8.58429015e-01, 2.96802800e-02, 1.11890651e-01],\n",
       "       [8.25128436e-01, 3.82498950e-02, 1.36621654e-01],\n",
       "       [5.17596900e-01, 1.33888707e-01, 3.48514348e-01],\n",
       "       [1.92257851e-01, 2.79001564e-01, 5.28740585e-01],\n",
       "       [5.88405132e-01, 1.09271623e-01, 3.02323282e-01],\n",
       "       [7.63006985e-01, 5.52368127e-02, 1.81756213e-01],\n",
       "       [1.98418662e-01, 2.75388569e-01, 5.26192725e-01],\n",
       "       [8.26364219e-01, 2.51955111e-02, 1.48440301e-01],\n",
       "       [3.13904285e-01, 2.08637670e-01, 4.77458090e-01],\n",
       "       [5.13234794e-01, 3.38878602e-01, 1.47886619e-01],\n",
       "       [4.86884147e-01, 9.52529237e-02, 4.17862892e-01],\n",
       "       [9.21398878e-01, 1.47015313e-02, 6.38995320e-02],\n",
       "       [9.78804171e-01, 3.09162377e-03, 1.81042347e-02],\n",
       "       [1.71888366e-01, 3.06039602e-01, 5.22072017e-01],\n",
       "       [9.49455261e-01, 8.69326107e-03, 4.18513976e-02],\n",
       "       [8.19181442e-01, 4.66736369e-02, 1.34144932e-01],\n",
       "       [2.47332588e-04, 9.95484591e-01, 4.26817825e-03],\n",
       "       [2.05570013e-05, 9.99635577e-01, 3.43832042e-04],\n",
       "       [1.44213453e-04, 9.97732162e-01, 2.12370371e-03],\n",
       "       [4.22492622e-06, 9.99904752e-01, 9.10811723e-05],\n",
       "       [1.41303361e-04, 9.98288333e-01, 1.57039159e-03],\n",
       "       [9.02330794e-05, 9.98513043e-01, 1.39670842e-03],\n",
       "       [2.26267366e-04, 9.97696579e-01, 2.07718904e-03],\n",
       "       [3.81245300e-05, 9.99394059e-01, 5.67816314e-04],\n",
       "       [3.40944638e-07, 9.99988079e-01, 1.15908460e-05],\n",
       "       [2.08442358e-07, 9.99991179e-01, 8.58431122e-06],\n",
       "       [2.90354407e-09, 9.99999642e-01, 3.08257910e-07],\n",
       "       [7.97986239e-02, 6.09983146e-01, 3.10218215e-01],\n",
       "       [1.31262164e-03, 9.75867093e-01, 2.28203684e-02],\n",
       "       [6.65177140e-08, 9.99995470e-01, 4.38472352e-06],\n",
       "       [1.66067730e-06, 9.99959707e-01, 3.86800348e-05],\n",
       "       [4.18422452e-08, 9.99997616e-01, 2.32805905e-06],\n",
       "       [3.64763923e-06, 9.99913812e-01, 8.25026000e-05],\n",
       "       [1.10984534e-01, 6.12727344e-01, 2.76288152e-01],\n",
       "       [2.19123121e-05, 9.99735296e-01, 2.42756287e-04],\n",
       "       [2.87276216e-05, 9.99220848e-01, 7.50476203e-04],\n",
       "       [7.33970031e-02, 5.87724209e-01, 3.38878810e-01],\n",
       "       [1.33396655e-01, 4.52137053e-01, 4.14466321e-01],\n",
       "       [5.88408718e-03, 9.33774829e-01, 6.03411458e-02],\n",
       "       [1.54858962e-01, 1.92650437e-01, 6.52490616e-01],\n",
       "       [5.13405614e-02, 6.75141573e-01, 2.73517877e-01],\n",
       "       [3.12843859e-01, 3.53736222e-01, 3.33419919e-01],\n",
       "       [3.07043525e-03, 9.69309866e-01, 2.76197959e-02],\n",
       "       [8.54936764e-02, 6.23751223e-01, 2.90755123e-01],\n",
       "       [5.94673418e-02, 3.69926393e-02, 9.03540015e-01],\n",
       "       [8.27857330e-02, 5.60439639e-02, 8.61170292e-01],\n",
       "       [3.23325230e-06, 9.99904752e-01, 9.20750463e-05],\n",
       "       [9.43943858e-02, 4.92644817e-01, 4.12960887e-01],\n",
       "       [1.45802519e-03, 9.76543725e-01, 2.19982862e-02],\n",
       "       [2.31713244e-07, 9.99989510e-01, 1.02958820e-05],\n",
       "       [1.27817032e-06, 9.99947309e-01, 5.13421655e-05],\n",
       "       [5.02858541e-07, 9.99982238e-01, 1.72410655e-05],\n",
       "       [1.04510353e-03, 9.87339377e-01, 1.16155744e-02],\n",
       "       [8.92181997e-05, 9.98440206e-01, 1.47058559e-03],\n",
       "       [1.69178552e-03, 9.86537457e-01, 1.17708230e-02],\n",
       "       [1.49417650e-02, 8.84549201e-01, 1.00509040e-01],\n",
       "       [2.12414496e-04, 9.97953773e-01, 1.83378987e-03],\n",
       "       [6.43081755e-10, 9.99999881e-01, 6.92563589e-08],\n",
       "       [4.55742367e-02, 7.16107905e-01, 2.38317937e-01],\n",
       "       [1.38254535e-07, 9.99990582e-01, 9.29984344e-06],\n",
       "       [1.76994437e-08, 9.99998808e-01, 1.23498774e-06],\n",
       "       [1.90450755e-05, 9.99528050e-01, 4.52864420e-04],\n",
       "       [3.13418627e-01, 2.16401026e-01, 4.70180362e-01],\n",
       "       [6.43222506e-07, 9.99974012e-01, 2.54034439e-05],\n",
       "       [1.46481427e-04, 9.98325169e-01, 1.52834214e-03],\n",
       "       [7.99330184e-04, 9.89719510e-01, 9.48122982e-03],\n",
       "       [2.37759352e-07, 9.99990225e-01, 9.57797965e-06],\n",
       "       [5.65451501e-06, 9.99900103e-01, 9.43245541e-05],\n",
       "       [7.13178364e-04, 9.90406215e-01, 8.88059009e-03],\n",
       "       [1.71230568e-08, 9.99999285e-01, 6.74161186e-07],\n",
       "       [1.34074748e-01, 1.48306116e-01, 7.17619121e-01],\n",
       "       [1.31484505e-03, 9.87747669e-01, 1.09373899e-02],\n",
       "       [9.43961728e-04, 9.91361558e-01, 7.69438688e-03],\n",
       "       [3.47417896e-04, 9.95138407e-01, 4.51404555e-03],\n",
       "       [1.80685952e-01, 2.49159425e-01, 5.70154667e-01],\n",
       "       [1.32938987e-02, 8.85695577e-01, 1.01010509e-01],\n",
       "       [4.92615345e-06, 9.99906540e-01, 8.85726913e-05],\n",
       "       [6.66145748e-03, 9.08391058e-01, 8.49474370e-02],\n",
       "       [2.40709037e-02, 8.29998076e-01, 1.45931080e-01],\n",
       "       [1.28347205e-03, 9.79707718e-01, 1.90087352e-02],\n",
       "       [3.75306746e-03, 9.48759913e-01, 4.74870838e-02],\n",
       "       [6.37265828e-07, 9.99967575e-01, 3.18120547e-05],\n",
       "       [7.83161886e-05, 9.98690069e-01, 1.23155094e-03],\n",
       "       [5.81818109e-04, 9.92302179e-01, 7.11592054e-03],\n",
       "       [2.11146306e-02, 8.55498075e-01, 1.23387307e-01],\n",
       "       [1.54006079e-01, 3.51121873e-01, 4.94872063e-01],\n",
       "       [2.20767930e-02, 8.37487817e-01, 1.40435413e-01],\n",
       "       [8.68012607e-02, 5.17989099e-02, 8.61399829e-01],\n",
       "       [3.03838192e-06, 9.99948502e-01, 4.85111086e-05],\n",
       "       [2.07516405e-05, 9.99664664e-01, 3.14643461e-04],\n",
       "       [7.93381969e-06, 9.99779999e-01, 2.12075276e-04],\n",
       "       [6.99920356e-02, 7.96117842e-01, 1.33890033e-01],\n",
       "       [3.31209041e-02, 7.90992141e-01, 1.75886944e-01],\n",
       "       [3.49140355e-05, 9.99326229e-01, 6.38890197e-04],\n",
       "       [3.68056030e-10, 1.00000000e+00, 4.56724045e-08],\n",
       "       [7.57670132e-05, 9.98074293e-01, 1.84989383e-03],\n",
       "       [5.74874648e-05, 9.98993456e-01, 9.49076668e-04],\n",
       "       [8.20114240e-02, 5.21177888e-01, 3.96810740e-01],\n",
       "       [1.29860622e-04, 9.97453272e-01, 2.41691433e-03],\n",
       "       [2.11589886e-05, 9.99598682e-01, 3.80139682e-04],\n",
       "       [1.12123229e-01, 8.87183920e-02, 7.99158394e-01],\n",
       "       [1.50422684e-05, 9.99709427e-01, 2.75563565e-04],\n",
       "       [4.73724038e-04, 9.96157110e-01, 3.36925033e-03],\n",
       "       [1.74742654e-01, 2.19026640e-01, 6.06230736e-01],\n",
       "       [1.63736373e-01, 3.86159658e-01, 4.50103939e-01],\n",
       "       [4.69544306e-02, 7.89251149e-01, 1.63794398e-01],\n",
       "       [7.83431824e-05, 9.98230517e-01, 1.69123919e-03],\n",
       "       [1.19335979e-07, 9.99992728e-01, 7.16006753e-06],\n",
       "       [3.65230881e-06, 9.99888539e-01, 1.07793232e-04],\n",
       "       [4.03202837e-04, 9.94880676e-01, 4.71605686e-03],\n",
       "       [2.04404641e-04, 9.96890128e-01, 2.90543633e-03],\n",
       "       [2.02485459e-11, 1.00000000e+00, 4.84756768e-09],\n",
       "       [9.71806422e-02, 5.06431282e-01, 3.96388084e-01],\n",
       "       [2.02353636e-04, 9.98455644e-01, 1.34196877e-03],\n",
       "       [1.19391307e-01, 4.12673473e-01, 4.67935264e-01],\n",
       "       [1.54658119e-06, 9.99964118e-01, 3.43117026e-05],\n",
       "       [3.20931633e-07, 9.99987721e-01, 1.19659617e-05],\n",
       "       [1.39067993e-01, 6.92814112e-01, 1.68117896e-01],\n",
       "       [4.79953087e-05, 9.99104083e-01, 8.47937830e-04],\n",
       "       [7.27741644e-05, 9.99161124e-01, 7.66093959e-04],\n",
       "       [2.61064561e-04, 9.96159434e-01, 3.57946637e-03],\n",
       "       [1.73030264e-06, 9.99964356e-01, 3.38435348e-05],\n",
       "       [7.43446697e-04, 9.90201771e-01, 9.05486662e-03],\n",
       "       [6.30761633e-06, 9.99843836e-01, 1.49771164e-04],\n",
       "       [8.21777037e-04, 9.87948298e-01, 1.12298271e-02],\n",
       "       [1.71117298e-09, 9.99999881e-01, 1.56876538e-07],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [1.82680152e-02, 8.76578391e-01, 1.05153598e-01],\n",
       "       [1.32071599e-01, 4.95013177e-01, 3.72915179e-01],\n",
       "       [1.12408483e-09, 9.99999881e-01, 7.52068985e-08],\n",
       "       [2.91766173e-06, 9.99886513e-01, 1.10640081e-04],\n",
       "       [2.32137663e-05, 9.99524951e-01, 4.51794069e-04],\n",
       "       [1.79428334e-05, 9.99735057e-01, 2.46925949e-04],\n",
       "       [5.45014736e-05, 9.99062836e-01, 8.82635592e-04],\n",
       "       [4.18605239e-08, 9.99997616e-01, 2.41155158e-06],\n",
       "       [3.31909541e-06, 9.99902487e-01, 9.42044935e-05],\n",
       "       [4.79706842e-03, 9.58858073e-01, 3.63449790e-02],\n",
       "       [5.34905894e-07, 9.99987125e-01, 1.24196595e-05],\n",
       "       [7.80220211e-01, 5.04065417e-02, 1.69373229e-01],\n",
       "       [5.85192709e-07, 9.99977589e-01, 2.18027326e-05],\n",
       "       [7.33898953e-04, 9.88682508e-01, 1.05836624e-02],\n",
       "       [1.17479509e-03, 9.86609399e-01, 1.22157317e-02],\n",
       "       [2.13094904e-06, 9.99888062e-01, 1.09785105e-04],\n",
       "       [2.39677320e-05, 9.99639034e-01, 3.37048026e-04],\n",
       "       [1.46442396e-03, 9.89865482e-01, 8.67018383e-03],\n",
       "       [2.05356322e-04, 9.96457398e-01, 3.33723170e-03],\n",
       "       [1.93925127e-02, 8.02238524e-01, 1.78368926e-01],\n",
       "       [2.20267611e-05, 9.99505997e-01, 4.71920474e-04],\n",
       "       [1.60792068e-01, 1.67044640e-01, 6.72163308e-01],\n",
       "       [4.64717465e-09, 9.99999642e-01, 3.22221382e-07],\n",
       "       [5.64849429e-08, 9.99997377e-01, 2.60920729e-06],\n",
       "       [9.63553433e-08, 9.99997020e-01, 2.91992433e-06],\n",
       "       [1.08510055e-01, 9.21526998e-02, 7.99337268e-01],\n",
       "       [3.27231479e-04, 9.95297611e-01, 4.37522866e-03],\n",
       "       [5.85426949e-02, 6.45082235e-01, 2.96375096e-01],\n",
       "       [1.04078138e-03, 9.91525590e-01, 7.43370084e-03],\n",
       "       [2.18604699e-01, 3.68970245e-01, 4.12425071e-01],\n",
       "       [9.54560819e-05, 9.98741210e-01, 1.16335077e-03],\n",
       "       [1.09277593e-07, 9.99992967e-01, 6.85673513e-06],\n",
       "       [5.41512221e-02, 7.54922867e-01, 1.90925896e-01],\n",
       "       [8.54727288e-04, 9.94640589e-01, 4.50457493e-03],\n",
       "       [7.27586951e-07, 9.99982119e-01, 1.72023356e-05],\n",
       "       [3.80832143e-02, 7.89619505e-01, 1.72297329e-01],\n",
       "       [2.87206508e-02, 1.10119339e-02, 9.60267425e-01],\n",
       "       [1.43602127e-04, 9.97686148e-01, 2.17031781e-03],\n",
       "       [2.24026844e-01, 4.44444627e-01, 3.31528544e-01],\n",
       "       [1.21167600e-02, 1.93185091e-03, 9.85951304e-01],\n",
       "       [1.81189761e-01, 2.52080321e-01, 5.66729903e-01],\n",
       "       [4.85440902e-03, 5.15492517e-04, 9.94630098e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [4.46900027e-03, 3.99912649e-04, 9.95131135e-01],\n",
       "       [1.73416808e-02, 4.93429741e-03, 9.77724016e-01],\n",
       "       [1.13923043e-01, 4.88871127e-01, 3.97205919e-01],\n",
       "       [9.73477960e-02, 6.49305657e-02, 8.37721646e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [1.57692581e-02, 3.18000652e-03, 9.81050789e-01],\n",
       "       [1.85262069e-01, 2.78864264e-01, 5.35873592e-01],\n",
       "       [7.09244004e-03, 9.17090237e-01, 7.58173689e-02],\n",
       "       [2.67614927e-02, 8.91657174e-03, 9.64321971e-01],\n",
       "       [1.04196258e-01, 4.98567760e-01, 3.97236049e-01],\n",
       "       [7.53684342e-03, 9.26308276e-04, 9.91536856e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [1.62408426e-01, 2.59396434e-03, 8.34997594e-01],\n",
       "       [1.53513893e-01, 3.85688663e-01, 4.60797369e-01],\n",
       "       [1.63614061e-02, 3.58690415e-03, 9.80051637e-01],\n",
       "       [4.35880236e-02, 1.75162349e-02, 9.38895702e-01],\n",
       "       [5.19535225e-03, 9.58933771e-01, 3.58708426e-02],\n",
       "       [4.11159219e-03, 3.93171562e-04, 9.95495200e-01],\n",
       "       [1.05165420e-02, 1.27572101e-03, 9.88207757e-01],\n",
       "       [9.20316391e-03, 1.41528668e-03, 9.89381552e-01],\n",
       "       [6.93994910e-02, 4.22954969e-02, 8.88305008e-01],\n",
       "       [4.10303473e-02, 1.75497420e-02, 9.41419959e-01],\n",
       "       [1.92183424e-02, 8.28025103e-01, 1.52756616e-01],\n",
       "       [3.43513757e-01, 1.79007217e-01, 4.77479011e-01],\n",
       "       [2.86844149e-02, 6.66809687e-03, 9.64647472e-01],\n",
       "       [1.77116692e-01, 2.30231479e-01, 5.92651844e-01],\n",
       "       [1.83947667e-01, 2.69499660e-01, 5.46552658e-01],\n",
       "       [7.14890733e-02, 4.42896923e-03, 9.24081922e-01],\n",
       "       [5.31702898e-02, 2.79601887e-02, 9.18869555e-01],\n",
       "       [2.47439444e-01, 2.48543143e-01, 5.04017413e-01],\n",
       "       [3.32273892e-03, 2.73172627e-04, 9.96404052e-01],\n",
       "       [7.52852708e-02, 5.80828846e-01, 3.43885869e-01],\n",
       "       [9.53461826e-02, 4.82174188e-01, 4.22479630e-01],\n",
       "       [1.64686501e-01, 1.79725438e-01, 6.55588090e-01],\n",
       "       [1.19499989e-01, 2.96761364e-01, 5.83738625e-01],\n",
       "       [9.80453007e-03, 1.78410939e-03, 9.88411427e-01],\n",
       "       [1.61041424e-01, 1.87960327e-01, 6.50998235e-01],\n",
       "       [3.67451645e-02, 1.33300973e-02, 9.49924767e-01],\n",
       "       [1.08544834e-01, 8.57671276e-02, 8.05688024e-01],\n",
       "       [1.78963437e-05, 9.99365866e-01, 6.16311445e-04],\n",
       "       [1.22683689e-01, 4.44448769e-01, 4.32867616e-01],\n",
       "       [1.06872648e-01, 1.04260780e-01, 7.88866520e-01],\n",
       "       [1.56965882e-01, 3.06032002e-01, 5.37002087e-01],\n",
       "       [2.29544174e-02, 7.78195977e-01, 1.98849633e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [3.61102708e-02, 1.17451269e-02, 9.52144563e-01],\n",
       "       [6.92665670e-03, 1.00318913e-03, 9.92070138e-01],\n",
       "       [6.02487475e-03, 9.24716771e-01, 6.92583472e-02],\n",
       "       [4.66836989e-02, 2.11591125e-02, 9.32157278e-01],\n",
       "       [3.24770063e-02, 7.62531340e-01, 2.04991624e-01],\n",
       "       [5.57609834e-02, 2.25764960e-02, 9.21662509e-01],\n",
       "       [6.13533631e-02, 3.74247581e-02, 9.01221931e-01],\n",
       "       [1.93774298e-01, 2.78106481e-01, 5.28119147e-01],\n",
       "       [4.79273964e-03, 4.70647850e-04, 9.94736612e-01],\n",
       "       [1.78859401e-02, 4.65561403e-03, 9.77458417e-01],\n",
       "       [3.02528515e-02, 2.68620640e-01, 7.01126516e-01],\n",
       "       [4.90652993e-02, 2.76736822e-02, 9.23260987e-01],\n",
       "       [3.61217707e-02, 1.18068643e-02, 9.52071309e-01],\n",
       "       [5.38490247e-03, 9.62174773e-01, 3.24402936e-02],\n",
       "       [2.84287799e-02, 9.27996449e-03, 9.62291300e-01],\n",
       "       [5.36594614e-02, 2.42671818e-02, 9.22073305e-01],\n",
       "       [2.14122701e-02, 2.82036676e-03, 9.75767434e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [6.08734414e-02, 2.83173602e-02, 9.10809159e-01],\n",
       "       [1.50282443e-01, 4.27321851e-01, 4.22395766e-01],\n",
       "       [1.66219518e-01, 2.25395471e-01, 6.08385026e-01],\n",
       "       [1.50854245e-01, 3.16459149e-01, 5.32686651e-01],\n",
       "       [1.06929764e-01, 7.67324045e-02, 8.16337883e-01],\n",
       "       [3.42259258e-02, 1.13053508e-02, 9.54468727e-01],\n",
       "       [1.27516329e-01, 1.10968061e-01, 7.61515617e-01],\n",
       "       [4.41867225e-02, 1.43553941e-02, 9.41457868e-01],\n",
       "       [5.46308309e-02, 5.61564744e-01, 3.83804381e-01],\n",
       "       [3.18586491e-02, 1.07532702e-02, 9.57388103e-01],\n",
       "       [4.44436856e-02, 1.08383531e-02, 9.44717944e-01],\n",
       "       [3.52286726e-01, 1.99024171e-01, 4.48689163e-01],\n",
       "       [1.88313834e-02, 2.89825257e-03, 9.78270411e-01],\n",
       "       [2.38979049e-02, 7.57709006e-03, 9.68524992e-01],\n",
       "       [1.56400144e-01, 3.34113747e-01, 5.09486079e-01],\n",
       "       [1.43562797e-02, 2.70305062e-03, 9.82940614e-01],\n",
       "       [1.00124158e-01, 6.81922585e-02, 8.31683636e-01],\n",
       "       [8.76327232e-02, 6.61626279e-01, 2.50741005e-01],\n",
       "       [1.17759079e-01, 2.09399052e-02, 8.61300945e-01],\n",
       "       [2.10027263e-01, 2.91003007e-02, 7.60872424e-01],\n",
       "       [2.30815541e-02, 8.00082088e-01, 1.76836357e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [1.26696885e-01, 1.93234041e-01, 6.80069029e-01],\n",
       "       [5.52725105e-05, 9.98722136e-01, 1.22255459e-03],\n",
       "       [1.55681089e-01, 1.51961654e-01, 6.92357242e-01],\n",
       "       [4.96856421e-01, 1.41428620e-01, 3.61714870e-01],\n",
       "       [3.73653144e-01, 1.89868361e-01, 4.36478496e-01],\n",
       "       [3.75801355e-01, 1.88962027e-01, 4.35236573e-01],\n",
       "       [1.61442176e-01, 1.69084042e-01, 6.69473827e-01],\n",
       "       [3.48883241e-01, 1.53906539e-01, 4.97210175e-01],\n",
       "       [8.73446390e-02, 7.36013055e-02, 8.39054048e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01],\n",
       "       [2.38311559e-01, 5.12864470e-01, 2.48824045e-01],\n",
       "       [1.85788915e-01, 2.82863587e-01, 5.31347513e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(validation_generator)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_classes = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21,   4,   7],\n",
       "       [  1, 132,  17],\n",
       "       [  1,  19,  81]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(validation_generator.classes, preds_classes) #, labels=[\"covid19\", \"normal\", \"pneumonia\"])\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     covid19       0.91      0.66      0.76        32\n",
      "      normal       0.85      0.88      0.87       150\n",
      "   pneumonia       0.77      0.80      0.79       101\n",
      "\n",
      "    accuracy                           0.83       283\n",
      "   macro avg       0.85      0.78      0.81       283\n",
      "weighted avg       0.83      0.83      0.83       283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(validation_generator.classes, preds_classes, target_names=[\"covid19\", \"normal\", \"pneumonia\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plot_confusion_matrix(cm = cf, \n",
    "                      normalize    = False,\n",
    "                      target_names = [\"covid19\", \"normal\", \"pneumonia\"],\n",
    "                      title        = \"\")\n",
    "plt.savefig('../docs/' + model.name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
